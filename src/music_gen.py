import numpy as np 
import music21
from music21 import converter, instrument, note, chord, stream
import torch
import torch.nn as nn
import glob
import pickle
from torch.utils.data import Dataset, DataLoader 

device = "cuda" if torch.cuda.is_available() else "cpu"


def create_midi(prediction_output):
    """ convert the output from the prediction to notes and create a midi file
        from the notes """
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)

    midi_stream.write('midi', fp='../outputs/test_output.mid')

def prepare_sequences(notes,vocab=[]):

    pitchnames = sorted(set(items for items in notes))
    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))
    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))
    print(note_to_int)
    print("Vocab size : ",len(note_to_int))
    data = [ note_to_int[note] for note in notes]
    print("total number of notes in dataset:", len(data))
    return data, note_to_int, int_to_note
    
class NotesDataset(Dataset):
    def __init__(self, noteslist):
        self.seq_len =  100
        no_seq = len(noteslist)//self.seq_len
        data=noteslist[:no_seq * self.seq_len]
        self.data=torch.tensor(data).view(-1,self.seq_len)

    def __len__(self):
        return self.data.shape[0]

    def __getitem__(self,index):
        arr = self.data[index]
        x = arr[:-1]
        y = arr[1:]
        return x,y

def collate(seq_list):
    inputs = torch.cat([s[0].unsqueeze(1) for s in seq_list],dim=1)
    targets = torch.cat([s[1].unsqueeze(1) for s in seq_list],dim=1)
    return inputs,targets


class MusicModel(nn.Module):
    def __init__(self,vocab_size,embed_size=400,hidden_size=1250,nlayers=2):
        super(MusicModel,self).__init__()
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.nlayers = nlayers
        self.embedding = nn.Embedding(vocab_size,embed_size)
        self.lstm = nn.LSTM(input_size=embed_size,hidden_size=hidden_size,num_layers=nlayers)
        self.fc_embed = nn.Linear(hidden_size,embed_size)
        #weight tying

    def forward(self,x,hidden=None,gen=False):
        # LX N
        batch_size = x.shape[1] #N
        embed = self.embedding(x) # L x N x E
        out_LSTM,hidden = self.lstm(embed,hidden) # 
        out_LSTM=out_LSTM.view(-1,self.hidden_size)#(L*N)x H
        out = self.fc_embed(out_LSTM) # (L*N) x E
        out = torch.mm(out,torch.transpose(self.embedding.weight,0,1)) #(L*N)*V
        out = out.view(-1,batch_size,vocab_size) # Lx N X V

        if gen==False:
            return out
        else:
            return out,hidden


def  train_batch(inputs,labels):
    inputs=inputs.to(device).long()
    labels=labels.to(device).long()
    output = model(inputs)
    output = output.view(-1,output.shape[2])
    loss = criterion(output,labels.view(-1))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()
    
def generate_notes(out_length,inputs):
    #model.test()
    inputs= torch.from_numpy(inputs).unsqueeze(1) #L x 1
    inputs= inputs.to(device).long()
    with torch.no_grad():
        generated_notes=[]
        output, hidden = model.forward(inputs,None,True)
        out=output[-1]
        _,curr_note = torch.max(out,dim=1) #1
        #print(curr_note)
        generated_notes.append(curr_note)

        i=1
        while(i<out_length):
            curr_note = curr_note.unsqueeze(0) #L=1 x N=1
            output,hidden = model.forward(curr_note,hidden,True)
            out =output[-1]
            _,curr_note = torch.max(out,dim=1)
            #print(curr_note)
            generated_notes.append(curr_note)
            i=i+1
        g = torch.cat(generated_notes,dim=0)
        #g = torch.transpose(g,0,1)
        g=g.tolist()
        return g



#prepare training data
notes = np.load('../data/notes', encoding = 'bytes' ) # list
data, note_to_int, int_to_note = prepare_sequences(notes) # list, dict
vocab_size=len(note_to_int)
num_epochs = 20
batch_size = 80

model = MusicModel(vocab_size)
model = model.to(device)

criterion=nn.CrossEntropyLoss()
optimizer= torch.optim.Adam(model.parameters(),weight_decay=1e-6)

#training
train_dataset = NotesDataset(data)
train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True,collate_fn=collate)
model.train()

for epoch in range(num_epochs):
    total_epoch_loss=0
    for batch_no ,(x,labels) in enumerate(train_dataloader):
        #print("x",x.shape)
        loss=train_batch(x,labels)
        total_epoch_loss+=loss
        print('Epoch:{}/{} batch_no:{}  Batch Loss:{:.4f}'.format(epoch+1,num_epochs,batch_no,loss))
    epoch_loss=total_epoch_loss/(batch_no+1)
    torch.save(model.state_dict(), "models/checkpoint.pt")
    print('------------------------')
    print('Epoch:{}/{} Epoch_Loss: {:.4f}'.format(epoch+1,num_epochs,epoch_loss))

randi = np.random.randint(0,len(data))
seed = np.array(data[randi:randi+10])
print (seed.shape)
gen_index = generate_notes(400,seed)
gen_notes= [int_to_note[i] for i in gen_index]
print (gen_notes)
create_midi(gen_notes)











        




    



